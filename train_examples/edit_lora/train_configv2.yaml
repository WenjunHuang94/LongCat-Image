# 1.Data setting
data_txt_root: '/home/disk2/hwj/LongCat-Image/data_example/text-to-image-2M-4000.txt'  # data csv_filepath
resolution: 512
aspect_ratio_type: 'mar_512'  # data bucketing strategy, mar_256、mar_512、mar_1024
null_text_ratio: 0.1
dataloader_num_workers: 4
train_batch_size: 8
repeats: 1

prompt_template_encode_prefix: "<|im_start|>system\nAs an image editing expert, first analyze the content and attributes of the input image(s). Then, based on the user's editing instructions, clearly and precisely determine how to modify the given image(s), ensuring that only the specified parts are altered and all other aspects remain consistent with the original(s).<|im_end|>\n<|im_start|>user\n<|vision_start|><|image_pad|><|vision_end|>"
prompt_template_encode_suffix: '<|im_end|>\n<|im_start|>assistant\n'
prompt_template_encode_start_idx: 67
prompt_template_encode_end_idx: 5

# 2. Model setting
text_tokenizer_max_length: 512  # tokenizer max len 
pretrained_model_name_or_path: "/home/disk2/hwj/my_hf_cache/hub/models--meituan-longcat--LongCat-Image-Edit/snapshots/7b54ef423aa7854be7861600024be5c56ab7875a" # root directory of the model，with vae、transformer、scheduler eta;
diffusion_pretrain_weight:  null # if a specified diffusion weight path is provided, load the model parameters from the current directory.
use_dynamic_shifting: true #  scheduler dynamic shifting
resume_from_checkpoint: latest
  # - "latest"  # Loads most recent step checkpoint
  # - "/path/to/checkpoint"  # Resumes from specified directory

# 3. Training setting
lora_rank: 64
use_ema: False
ema_rate: 0.999
mixed_precision: 'bf16'
max_train_steps: 2500
gradient_accumulation_steps: 1
gradient_checkpointing: true
gradient_clip: 1.0
learning_rate: 2.0e-4
adam_weight_decay: 1.0e-2
adam_epsilon: 1.0e-8
adam_beta1: 0.9
adam_beta2: 0.999
lr_num_cycles: 1
lr_power: 1.0
lr_scheduler: 'constant_with_warmup'
lr_warmup_steps: 100

#4. Log setting
log_interval: 10
save_model_steps: 200
work_dir: 'output/edit_lora_model-512-v3'
seed: 43